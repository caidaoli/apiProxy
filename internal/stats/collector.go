package stats

import (
	"context"
	"fmt"
	"log"
	"math"
	"runtime"
	"strconv"
	"sync"
	"sync/atomic"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/redis/go-redis/v9"
)

// Rediså­˜å‚¨ç›¸å…³å¸¸é‡
const (
	KeyStatsCounters       = "api_proxy:stats:counters"
	KeyStatsEndpointPrefix = "api_proxy:stats:endpoints:"
)

// EndpointStats ç«¯ç‚¹ç»Ÿè®¡ä¿¡æ¯
type EndpointStats struct {
	Total int64 `json:"total"`
	Today int64 `json:"today"`
	Week  int64 `json:"week"`
	Month int64 `json:"month"`
}

// Request è¯·æ±‚è®°å½•
type Request struct {
	Endpoint  string `json:"endpoint"`
	Timestamp int64  `json:"timestamp"`
}

// TimeWindow æ—¶é—´çª—å£ç»Ÿè®¡
type TimeWindow struct {
	mu          sync.RWMutex
	counters    map[string]*atomic.Int64
	requests    []Request
	lastCleanup time.Time
}

// Stats ç»Ÿè®¡ç®¡ç†å™¨
type Stats struct {
	mu         sync.RWMutex
	Total      int64                     `json:"total"`
	Endpoints  map[string]*EndpointStats `json:"endpoints"`
	timeWindow *TimeWindow
	lastUpdate time.Time
}

// PerformanceMetrics æ€§èƒ½æŒ‡æ ‡
type PerformanceMetrics struct {
	mu              sync.RWMutex
	RequestsPerSec  float64 `json:"requests_per_sec"`
	AvgResponseTime int64   `json:"avg_response_time_ms"`
	ErrorRate       float64 `json:"error_rate"`
	MemoryUsageMB   float64 `json:"memory_usage_mb"`
	GoroutineCount  int     `json:"goroutine_count"`
	LastUpdated     int64   `json:"last_updated"`
}

// Collector ç»Ÿè®¡æ”¶é›†å™¨
type Collector struct {
	stats             *Stats
	perfMetrics       *PerformanceMetrics
	requestCount      int64
	errorCount        int64
	responseTimeSum   int64
	responseTimeCount int64
	lastQPSUpdate     int64
	lastRequestCount  int64
	redisClient       *redis.Client // Rediså®¢æˆ·ç«¯ç”¨äºæŒä¹…åŒ–
}

// NewCollector åˆ›å»ºç»Ÿè®¡æ”¶é›†å™¨
func NewCollector(redisClient *redis.Client) *Collector {
	c := &Collector{
		stats: &Stats{
			Endpoints: make(map[string]*EndpointStats),
			timeWindow: &TimeWindow{
				counters: make(map[string]*atomic.Int64),
				requests: make([]Request, 0, 1000),
			},
			lastUpdate: time.Now(),
		},
		perfMetrics: &PerformanceMetrics{
			LastUpdated: time.Now().UnixMilli(),
		},
		redisClient: redisClient,
	}

	// ä»RedisåŠ è½½å†å²ç»Ÿè®¡æ•°æ®
	if redisClient != nil {
		if err := c.LoadFromRedis(context.Background()); err != nil {
			log.Printf("âš ï¸  Failed to load stats from Redis: %v (starting with fresh stats)", err)
		} else {
			log.Println("âœ… ç»Ÿè®¡æ•°æ®å·²ä»Redisæ¢å¤")
		}
	}

	// å¯åŠ¨ç»Ÿè®¡æ›´æ–°åç¨‹
	go func() {
		ticker := time.NewTicker(3 * time.Second)
		defer ticker.Stop()
		for range ticker.C {
			c.stats.updateSummaryStats()
		}
	}()

	// å¯åŠ¨æ€§èƒ½æŒ‡æ ‡æ›´æ–°åç¨‹
	go func() {
		ticker := time.NewTicker(5 * time.Second)
		defer ticker.Stop()
		for range ticker.C {
			c.updatePerformanceMetrics()
		}
	}()

	// å¯åŠ¨å®šæ—¶ä¿å­˜åˆ°Redisçš„åç¨‹
	if redisClient != nil {
		go func() {
			ticker := time.NewTicker(1 * time.Minute)
			defer ticker.Stop()
			for range ticker.C {
				if err := c.SaveToRedis(context.Background()); err != nil {
					log.Printf("âŒ Failed to save stats to Redis: %v", err)
				} else {
					log.Println("ğŸ’¾ ç»Ÿè®¡æ•°æ®å·²ä¿å­˜åˆ°Redis")
				}
			}
		}()
		log.Println("ğŸ”„ ç»Ÿè®¡æ•°æ®è‡ªåŠ¨ä¿å­˜å·²å¯ç”¨ (æ¯1åˆ†é’Ÿ)")
	}

	log.Println("ğŸ“Š ç»Ÿè®¡æ”¶é›†å™¨å·²åˆå§‹åŒ–")
	return c
}

// InitializeEndpoints åˆå§‹åŒ–ç«¯ç‚¹ç»Ÿè®¡
func (c *Collector) InitializeEndpoints(endpoints []string) {
	c.stats.mu.Lock()
	defer c.stats.mu.Unlock()

	for _, endpoint := range endpoints {
		if _, exists := c.stats.Endpoints[endpoint]; !exists {
			c.stats.Endpoints[endpoint] = &EndpointStats{}
			c.stats.timeWindow.counters[endpoint] = &atomic.Int64{}
		}
	}
	log.Printf("ğŸ“Š å·²åˆå§‹åŒ– %d ä¸ªç«¯ç‚¹çš„ç»Ÿè®¡", len(endpoints))
}

// RecordRequest è®°å½•è¯·æ±‚
func (c *Collector) RecordRequest(endpoint string) {
	// ç¡®ä¿ç«¯ç‚¹å­˜åœ¨
	c.stats.mu.RLock()
	counter, exists := c.stats.timeWindow.counters[endpoint]
	c.stats.mu.RUnlock()

	if !exists {
		// åŠ¨æ€æ·»åŠ æ–°ç«¯ç‚¹
		c.stats.mu.Lock()
		if _, exists := c.stats.timeWindow.counters[endpoint]; !exists {
			c.stats.Endpoints[endpoint] = &EndpointStats{}
			c.stats.timeWindow.counters[endpoint] = &atomic.Int64{}
			counter = c.stats.timeWindow.counters[endpoint]
		}
		c.stats.mu.Unlock()
	}

	// åŸå­æ“ä½œæ›´æ–°è®¡æ•°å™¨
	if counter != nil {
		counter.Add(1)
	}

	// å¼‚æ­¥æ·»åŠ è¯¦ç»†è®°å½•
	go func() {
		c.stats.timeWindow.mu.Lock()
		defer c.stats.timeWindow.mu.Unlock()

		c.stats.timeWindow.requests = append(c.stats.timeWindow.requests, Request{
			Endpoint:  endpoint,
			Timestamp: time.Now().Unix(),
		})

		c.cleanupOldRequests()
	}()
}

// cleanupOldRequests æ¸…ç†æ—§è¯·æ±‚è®°å½•
func (c *Collector) cleanupOldRequests() {
	now := time.Now()
	if now.Sub(c.stats.timeWindow.lastCleanup) < 5*time.Minute {
		return
	}

	cutoff := now.Add(-30 * 24 * time.Hour).Unix()
	var newRequests []Request

	for _, req := range c.stats.timeWindow.requests {
		if req.Timestamp > cutoff {
			newRequests = append(newRequests, req)
		}
	}

	if len(newRequests) > 500 {
		newRequests = newRequests[len(newRequests)-500:]
	}

	c.stats.timeWindow.requests = newRequests
	c.stats.timeWindow.lastCleanup = now
}

// updateSummaryStats æ›´æ–°æ±‡æ€»ç»Ÿè®¡
func (s *Stats) updateSummaryStats() {
	s.mu.Lock()
	defer s.mu.Unlock()

	now := time.Now()
	today := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location()).Unix()
	weekAgo := now.AddDate(0, 0, -7).Unix()
	monthAgo := now.AddDate(0, -1, 0).Unix()

	for _, endpointStats := range s.Endpoints {
		atomic.StoreInt64(&endpointStats.Today, 0)
		atomic.StoreInt64(&endpointStats.Week, 0)
		atomic.StoreInt64(&endpointStats.Month, 0)
	}

	totalRequests := int64(0)
	for endpoint, counter := range s.timeWindow.counters {
		if endpointStats, exists := s.Endpoints[endpoint]; exists {
			total := counter.Load()
			atomic.StoreInt64(&endpointStats.Total, total)
			totalRequests += total
		}
	}

	s.Total = totalRequests

	s.timeWindow.mu.RLock()
	for _, req := range s.timeWindow.requests {
		if endpointStats, exists := s.Endpoints[req.Endpoint]; exists {
			if req.Timestamp >= today {
				atomic.AddInt64(&endpointStats.Today, 1)
			}
			if req.Timestamp >= weekAgo {
				atomic.AddInt64(&endpointStats.Week, 1)
			}
			if req.Timestamp >= monthAgo {
				atomic.AddInt64(&endpointStats.Month, 1)
			}
		}
	}
	s.timeWindow.mu.RUnlock()

	s.lastUpdate = now
}

// getStatsSnapshot è·å–ç»Ÿè®¡å¿«ç…§
func (s *Stats) getStatsSnapshot() *Stats {
	s.mu.RLock()
	defer s.mu.RUnlock()

	snapshot := &Stats{
		Total:     s.Total,
		Endpoints: make(map[string]*EndpointStats),
	}

	for endpoint, endpointStats := range s.Endpoints {
		snapshot.Endpoints[endpoint] = &EndpointStats{
			Total: atomic.LoadInt64(&endpointStats.Total),
			Today: atomic.LoadInt64(&endpointStats.Today),
			Week:  atomic.LoadInt64(&endpointStats.Week),
			Month: atomic.LoadInt64(&endpointStats.Month),
		}
	}

	return snapshot
}

// updatePerformanceMetrics æ›´æ–°æ€§èƒ½æŒ‡æ ‡
func (c *Collector) updatePerformanceMetrics() {
	c.perfMetrics.mu.Lock()
	defer c.perfMetrics.mu.Unlock()

	now := time.Now()

	var m runtime.MemStats
	runtime.ReadMemStats(&m)

	c.perfMetrics.MemoryUsageMB = math.Round(float64(m.Alloc)/1024/1024*100) / 100
	c.perfMetrics.GoroutineCount = runtime.NumGoroutine()
	c.perfMetrics.LastUpdated = now.UnixMilli()

	totalReqs := int64(0)
	c.stats.mu.RLock()
	for _, counter := range c.stats.timeWindow.counters {
		totalReqs += counter.Load()
	}
	c.stats.mu.RUnlock()

	// è®¡ç®—QPS
	currentTime := now.Unix()
	lastUpdate := atomic.LoadInt64(&c.lastQPSUpdate)
	currentRequests := atomic.LoadInt64(&c.requestCount)

	if lastUpdate == 0 {
		atomic.StoreInt64(&c.lastQPSUpdate, currentTime)
		atomic.StoreInt64(&c.lastRequestCount, currentRequests)
		c.perfMetrics.RequestsPerSec = 0.0
	} else {
		timeDiff := currentTime - lastUpdate
		if timeDiff > 0 {
			lastReqs := atomic.LoadInt64(&c.lastRequestCount)
			requestDiff := currentRequests - lastReqs

			qps := float64(requestDiff) / float64(timeDiff)

			if c.perfMetrics.RequestsPerSec == 0 {
				c.perfMetrics.RequestsPerSec = qps
			} else {
				c.perfMetrics.RequestsPerSec = 0.3*qps + 0.7*c.perfMetrics.RequestsPerSec
			}

			c.perfMetrics.RequestsPerSec = math.Round(c.perfMetrics.RequestsPerSec*100) / 100

			atomic.StoreInt64(&c.lastQPSUpdate, currentTime)
			atomic.StoreInt64(&c.lastRequestCount, currentRequests)
		}
	}

	// è®¡ç®—é”™è¯¯ç‡
	totalErrors := atomic.LoadInt64(&c.errorCount)
	if totalReqs > 0 {
		errorRate := float64(totalErrors) / float64(totalReqs) * 100
		c.perfMetrics.ErrorRate = math.Round(errorRate*100) / 100
	}

	// è®¡ç®—å¹³å‡å“åº”æ—¶é—´
	totalResponseTime := atomic.LoadInt64(&c.responseTimeSum)
	responseCount := atomic.LoadInt64(&c.responseTimeCount)
	if responseCount > 0 {
		c.perfMetrics.AvgResponseTime = totalResponseTime / responseCount
		if responseCount > 1000 {
			atomic.StoreInt64(&c.responseTimeSum, 0)
			atomic.StoreInt64(&c.responseTimeCount, 0)
		}
	}
}

// HandleStats å¤„ç†ç»Ÿè®¡APIè¯·æ±‚
func (c *Collector) HandleStats(ctx *gin.Context) {
	ctx.Header("Access-Control-Allow-Origin", "*")
	ctx.Header("Access-Control-Allow-Methods", "GET, OPTIONS")
	ctx.Header("Access-Control-Allow-Headers", "Content-Type")

	if ctx.Request.Method == "OPTIONS" {
		ctx.Status(204)
		return
	}

	snapshot := c.stats.getStatsSnapshot()

	c.stats.timeWindow.mu.RLock()
	requests := make([]Request, len(c.stats.timeWindow.requests))
	copy(requests, c.stats.timeWindow.requests)
	c.stats.timeWindow.mu.RUnlock()

	c.perfMetrics.mu.RLock()
	response := gin.H{
		"total":     snapshot.Total,
		"endpoints": snapshot.Endpoints,
		"requests":  requests,
		"performance": gin.H{
			"requests_per_sec":     c.perfMetrics.RequestsPerSec,
			"avg_response_time_ms": c.perfMetrics.AvgResponseTime,
			"error_rate":           c.perfMetrics.ErrorRate,
			"memory_usage_mb":      c.perfMetrics.MemoryUsageMB,
			"goroutine_count":      c.perfMetrics.GoroutineCount,
			"last_updated":         c.perfMetrics.LastUpdated,
		},
	}
	c.perfMetrics.mu.RUnlock()

	ctx.JSON(200, response)
}

// GetErrorCount è·å–é”™è¯¯è®¡æ•°å™¨æŒ‡é’ˆ
func (c *Collector) GetErrorCount() *int64 {
	return &c.errorCount
}

// GetRequestCount è·å–è¯·æ±‚è®¡æ•°å™¨æŒ‡é’ˆ
func (c *Collector) GetRequestCount() *int64 {
	return &c.requestCount
}

// UpdateResponseMetrics æ›´æ–°å“åº”æŒ‡æ ‡
func (c *Collector) UpdateResponseMetrics(responseTime int64) {
	atomic.AddInt64(&c.responseTimeSum, responseTime)
	atomic.AddInt64(&c.responseTimeCount, 1)
}

// SaveToRedis ä¿å­˜ç»Ÿè®¡æ•°æ®åˆ°Redis
func (c *Collector) SaveToRedis(ctx context.Context) error {
	if c.redisClient == nil {
		return fmt.Errorf("redis client is not initialized")
	}

	pipe := c.redisClient.Pipeline()

	// ä¿å­˜å…¨å±€è®¡æ•°å™¨
	counters := map[string]interface{}{
		"request_count":       atomic.LoadInt64(&c.requestCount),
		"error_count":         atomic.LoadInt64(&c.errorCount),
		"response_time_sum":   atomic.LoadInt64(&c.responseTimeSum),
		"response_time_count": atomic.LoadInt64(&c.responseTimeCount),
		"total":               c.stats.Total,
		"last_update":         time.Now().Unix(),
	}
	pipe.HSet(ctx, KeyStatsCounters, counters)

	// ä¿å­˜æ¯ä¸ªendpointçš„ç»Ÿè®¡æ•°æ®
	c.stats.mu.RLock()
	for prefix, stats := range c.stats.Endpoints {
		endpointKey := KeyStatsEndpointPrefix + prefix
		endpointData := map[string]interface{}{
			"total": stats.Total,
			"today": stats.Today,
			"week":  stats.Week,
			"month": stats.Month,
		}
		pipe.HSet(ctx, endpointKey, endpointData)
	}
	c.stats.mu.RUnlock()

	// æ‰§è¡Œæ‰¹é‡æ“ä½œ
	_, err := pipe.Exec(ctx)
	return err
}

// LoadFromRedis ä»RedisåŠ è½½ç»Ÿè®¡æ•°æ®
func (c *Collector) LoadFromRedis(ctx context.Context) error {
	if c.redisClient == nil {
		return fmt.Errorf("redis client is not initialized")
	}

	// åŠ è½½å…¨å±€è®¡æ•°å™¨
	counters, err := c.redisClient.HGetAll(ctx, KeyStatsCounters).Result()
	if err != nil {
		return fmt.Errorf("failed to load counters: %w", err)
	}

	// å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›nilï¼ˆä¸æ˜¯é”™è¯¯ï¼‰
	if len(counters) == 0 {
		return nil
	}

	// æ¢å¤è®¡æ•°å™¨
	if val, ok := counters["request_count"]; ok {
		if count, err := strconv.ParseInt(val, 10, 64); err == nil {
			atomic.StoreInt64(&c.requestCount, count)
		}
	}
	if val, ok := counters["error_count"]; ok {
		if count, err := strconv.ParseInt(val, 10, 64); err == nil {
			atomic.StoreInt64(&c.errorCount, count)
		}
	}
	if val, ok := counters["response_time_sum"]; ok {
		if sum, err := strconv.ParseInt(val, 10, 64); err == nil {
			atomic.StoreInt64(&c.responseTimeSum, sum)
		}
	}
	if val, ok := counters["response_time_count"]; ok {
		if count, err := strconv.ParseInt(val, 10, 64); err == nil {
			atomic.StoreInt64(&c.responseTimeCount, count)
		}
	}
	if val, ok := counters["total"]; ok {
		if total, err := strconv.ParseInt(val, 10, 64); err == nil {
			c.stats.Total = total
		}
	}

	// åŠ è½½æ‰€æœ‰endpointç»Ÿè®¡æ•°æ®
	keys, err := c.redisClient.Keys(ctx, KeyStatsEndpointPrefix+"*").Result()
	if err != nil {
		return fmt.Errorf("failed to get endpoint keys: %w", err)
	}

	c.stats.mu.Lock()
	defer c.stats.mu.Unlock()

	for _, key := range keys {
		prefix := key[len(KeyStatsEndpointPrefix):]
		data, err := c.redisClient.HGetAll(ctx, key).Result()
		if err != nil {
			log.Printf("âš ï¸  Failed to load stats for endpoint %s: %v", prefix, err)
			continue
		}

		stats := &EndpointStats{}
		if val, ok := data["total"]; ok {
			if total, err := strconv.ParseInt(val, 10, 64); err == nil {
				stats.Total = total
			}
		}
		if val, ok := data["today"]; ok {
			if today, err := strconv.ParseInt(val, 10, 64); err == nil {
				stats.Today = today
			}
		}
		if val, ok := data["week"]; ok {
			if week, err := strconv.ParseInt(val, 10, 64); err == nil {
				stats.Week = week
			}
		}
		if val, ok := data["month"]; ok {
			if month, err := strconv.ParseInt(val, 10, 64); err == nil {
				stats.Month = month
			}
		}

		c.stats.Endpoints[prefix] = stats
	}

	loadedEndpoints := len(keys)
	log.Printf("ğŸ“Š ä»Redisæ¢å¤äº† %d ä¸ªendpointçš„ç»Ÿè®¡æ•°æ®", loadedEndpoints)

	return nil
}
